{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# ============== CONFIGURACI√ìN ==============\n",
    "DATASET_PATH = r\"C:\\Users\\Usuario\\.cache\\kagglehub\\datasets\\philosopher0808\\real-vs-ai-generated-faces-dataset\\versions\\1\\dataset\\dataset\"\n",
    "IMG_SIZE = 224  # Tama√±o √≥ptimo para transfer learning\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS = 50\n",
    "\n",
    "# ============== DATA GENERATORS ==============\n",
    "print(\"Configurando generadores de datos...\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.8, 1.2]  # Variaci√≥n de brillo\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=f\"{DATASET_PATH}/train\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    directory=f\"{DATASET_PATH}/validate\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    directory=f\"{DATASET_PATH}/test\",\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Guardar nombres de clases\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "print(f\"Clases detectadas: {class_names}\")\n",
    "print(f\"Total im√°genes entrenamiento: {train_generator.samples}\")\n",
    "print(f\"Total im√°genes validaci√≥n: {val_generator.samples}\")\n",
    "print(f\"Total im√°genes test: {test_generator.samples}\")\n",
    "\n",
    "# ============== CREAR MODELO CON TRANSFER LEARNING ==============\n",
    "print(\"\\nCreando modelo con MobileNetV2...\")\n",
    "\n",
    "# Cargar modelo base preentrenado\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Congelar las capas del modelo base inicialmente\n",
    "base_model.trainable = False\n",
    "\n",
    "# Crear el modelo completo\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "], name='AI_Image_Detector')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ============== COMPILAR MODELO ==============\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# ============== CALLBACKS ==============\n",
    "callbacks = [\n",
    "    # Detener si no mejora en 7 √©pocas\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Guardar el mejor modelo\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Reducir learning rate si se estanca\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ============== ENTRENAMIENTO ==============\n",
    "print(\"\\n========== FASE 1: Entrenamiento con capas congeladas ==========\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============== FINE-TUNING ==============\n",
    "print(\"\\n========== FASE 2: Fine-tuning (descongelando capas) ==========\")\n",
    "\n",
    "# Descongelar las √∫ltimas capas del modelo base\n",
    "base_model.trainable = True\n",
    "\n",
    "# Congelar solo las primeras capas\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompilar con learning rate m√°s bajo\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# Continuar entrenamiento\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    initial_epoch=len(history.history['loss']),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Combinar historiales\n",
    "for key in history.history.keys():\n",
    "    history.history[key].extend(history_fine.history[key])\n",
    "\n",
    "# ============== EVALUACI√ìN ==============\n",
    "print(\"\\n========== EVALUACI√ìN EN TEST SET ==========\")\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Guardar modelo final\n",
    "model.save('final_model.keras')\n",
    "print(\"\\nModelo guardado como 'final_model.keras'\")\n",
    "\n",
    "# ============== VISUALIZACI√ìN DE RESULTADOS ==============\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], 'b', label='Train')\n",
    "axes[0].plot(history.history['val_accuracy'], 'r', label='Validation')\n",
    "axes[0].set_xlabel('√âpocas')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy durante entrenamiento')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], 'b', label='Train')\n",
    "axes[1].plot(history.history['val_loss'], 'r', label='Validation')\n",
    "axes[1].set_xlabel('√âpocas')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Loss durante entrenamiento')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============== FUNCI√ìN DE PREDICCI√ìN ==============\n",
    "def predict_image(img_path, model, class_names):\n",
    "    \"\"\"\n",
    "    Predice si una imagen es real o generada por IA\n",
    "    \n",
    "    Args:\n",
    "        img_path: Ruta de la imagen\n",
    "        model: Modelo entrenado\n",
    "        class_names: Lista con nombres de las clases\n",
    "        \n",
    "    Returns:\n",
    "        predicted_class: Clase predicha\n",
    "        confidence: Confianza de la predicci√≥n (%)\n",
    "    \"\"\"\n",
    "    # Cargar y preprocesar imagen\n",
    "    img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Predecir\n",
    "    prediction = model.predict(img_array, verbose=0)\n",
    "    predicted_idx = np.argmax(prediction)\n",
    "    predicted_class = class_names[predicted_idx]\n",
    "    confidence = prediction[0][predicted_idx] * 100\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Predicci√≥n: {predicted_class}\\nConfianza: {confidence:.2f}%\", \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # A√±adir barra de probabilidades\n",
    "    plt.text(10, img.size[1] - 10, \n",
    "             f\"{class_names[0]}: {prediction[0][0]*100:.1f}%\\n{class_names[1]}: {prediction[0][1]*100:.1f}%\",\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "             fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Imagen: {img_path}\")\n",
    "    print(f\"Predicci√≥n: {predicted_class}\")\n",
    "    print(f\"Confianza: {confidence:.2f}%\")\n",
    "    print(f\"Probabilidades detalladas:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"  - {class_name}: {prediction[0][i]*100:.2f}%\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# ============== PREDICCIONES DE EJEMPLO ==============\n",
    "print(\"\\n========== PREDICCIONES EN IM√ÅGENES DE PRUEBA ==========\")\n",
    "\n",
    "# Lista de im√°genes a probar\n",
    "test_images = [\n",
    "    r\"C:\\Users\\Usuario\\Downloads\\IA1.jpg\",\n",
    "    r\"C:\\Users\\Usuario\\Downloads\\IA2.jpg\",\n",
    "    r\"C:\\Users\\Usuario\\Downloads\\IA3.jpg\",\n",
    "    r\"C:\\Users\\Usuario\\Downloads\\IA4.jpg\"\n",
    "]\n",
    "\n",
    "# Predecir cada imagen\n",
    "results = []\n",
    "for img_path in test_images:\n",
    "    try:\n",
    "        predicted_class, confidence = predict_image(img_path, model, class_names)\n",
    "        results.append({\n",
    "            'path': img_path,\n",
    "            'prediction': predicted_class,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar {img_path}: {e}\")\n",
    "\n",
    "# Resumen de resultados\n",
    "print(\"\\n========== RESUMEN DE PREDICCIONES ==========\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['path'].split('\\\\')[-1]}: {result['prediction']} ({result['confidence']:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úì Proceso completado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ff9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# ============== CONFIGURACI√ìN ==============\n",
    "import os\n",
    "\n",
    "DATASET_PATH = r\"C:\\Users\\Usuario\\.cache\\kagglehub\\datasets\\philosopher0808\\real-vs-ai-generated-faces-dataset\\versions\\1\\dataset\\dataset\"\n",
    "IMG_SIZE = 224  # Tama√±o √≥ptimo para transfer learning\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS = 50\n",
    "\n",
    "# ============== VERIFICAR Y EXPLORAR DATASET ==============\n",
    "print(\"Verificando estructura del dataset...\")\n",
    "\n",
    "# Verificar si existe el directorio base\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚ùå ERROR: No existe el directorio: {DATASET_PATH}\")\n",
    "    print(\"\\nBuscando la ruta correcta...\")\n",
    "    \n",
    "    # Intentar encontrar el dataset\n",
    "    base_path = r\"C:\\Users\\Usuario\\.cache\\kagglehub\\datasets\\philosopher0808\"\n",
    "    if os.path.exists(base_path):\n",
    "        print(f\"‚úì Directorio base encontrado: {base_path}\")\n",
    "        print(\"\\nContenido del directorio:\")\n",
    "        for root, dirs, files in os.walk(base_path):\n",
    "            level = root.replace(base_path, '').count(os.sep)\n",
    "            indent = ' ' * 2 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            subindent = ' ' * 2 * (level + 1)\n",
    "            for file in files[:5]:  # Solo primeros 5 archivos\n",
    "                print(f\"{subindent}{file}\")\n",
    "            if len(files) > 5:\n",
    "                print(f\"{subindent}... y {len(files)-5} archivos m√°s\")\n",
    "    else:\n",
    "        print(f\"‚ùå No se encuentra ni siquiera: {base_path}\")\n",
    "        print(\"\\nüí° Soluci√≥n: Verifica que hayas descargado el dataset correctamente desde Kaggle\")\n",
    "    \n",
    "    raise FileNotFoundError(f\"No se encuentra el dataset en {DATASET_PATH}\")\n",
    "\n",
    "print(f\"‚úì Directorio base existe: {DATASET_PATH}\")\n",
    "\n",
    "# Explorar estructura\n",
    "print(\"\\nEstructura del dataset:\")\n",
    "for item in os.listdir(DATASET_PATH):\n",
    "    item_path = os.path.join(DATASET_PATH, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        print(f\"  üìÅ {item}/\")\n",
    "        # Ver contenido de subdirectorios\n",
    "        try:\n",
    "            subdirs = os.listdir(item_path)\n",
    "            for subdir in subdirs[:5]:  # Primeros 5\n",
    "                subdir_path = os.path.join(item_path, subdir)\n",
    "                if os.path.isdir(subdir_path):\n",
    "                    n_files = len([f for f in os.listdir(subdir_path) if os.path.isfile(os.path.join(subdir_path, f))])\n",
    "                    print(f\"      üìÇ {subdir}/ ({n_files} archivos)\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Detectar si los subdirectorios son train/validate/test o training/validation/testing\n",
    "possible_train_names = ['train', 'training', 'Train', 'TRAIN']\n",
    "possible_val_names = ['validate', 'validation', 'val', 'Validate', 'VALIDATE']\n",
    "possible_test_names = ['test', 'testing', 'Test', 'TEST']\n",
    "\n",
    "train_dir = None\n",
    "val_dir = None\n",
    "test_dir = None\n",
    "\n",
    "for item in os.listdir(DATASET_PATH):\n",
    "    item_lower = item.lower()\n",
    "    if item_lower in [n.lower() for n in possible_train_names]:\n",
    "        train_dir = os.path.join(DATASET_PATH, item)\n",
    "    elif item_lower in [n.lower() for n in possible_val_names]:\n",
    "        val_dir = os.path.join(DATASET_PATH, item)\n",
    "    elif item_lower in [n.lower() for n in possible_test_names]:\n",
    "        test_dir = os.path.join(DATASET_PATH, item)\n",
    "\n",
    "if not all([train_dir, val_dir, test_dir]):\n",
    "    print(\"\\n‚ö†Ô∏è  ADVERTENCIA: No se encontraron los directorios esperados (train/validate/test)\")\n",
    "    print(\"Directorios encontrados:\")\n",
    "    print(f\"  Train: {train_dir if train_dir else '‚ùå NO ENCONTRADO'}\")\n",
    "    print(f\"  Validation: {val_dir if val_dir else '‚ùå NO ENCONTRADO'}\")\n",
    "    print(f\"  Test: {test_dir if test_dir else '‚ùå NO ENCONTRADO'}\")\n",
    "    print(\"\\nüí° El dataset puede tener una estructura diferente.\")\n",
    "    print(\"Por favor, indica cu√°l es la estructura correcta del dataset.\")\n",
    "    raise FileNotFoundError(\"Estructura de directorios no compatible\")\n",
    "\n",
    "print(f\"\\n‚úì Directorios encontrados:\")\n",
    "print(f\"  Train: {train_dir}\")\n",
    "print(f\"  Validation: {val_dir}\")\n",
    "print(f\"  Test: {test_dir}\")\n",
    "\n",
    "# ============== DATA GENERATORS ==============\n",
    "print(\"\\nConfigurando generadores de datos...\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.8, 1.2]  # Variaci√≥n de brillo\n",
    ")\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_dir,  # Usar la ruta detectada autom√°ticamente\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    directory=val_dir,  # Usar la ruta detectada autom√°ticamente\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    directory=test_dir,  # Usar la ruta detectada autom√°ticamente\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Guardar nombres de clases\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "print(f\"Clases detectadas: {class_names}\")\n",
    "print(f\"Total im√°genes entrenamiento: {train_generator.samples}\")\n",
    "print(f\"Total im√°genes validaci√≥n: {val_generator.samples}\")\n",
    "print(f\"Total im√°genes test: {test_generator.samples}\")\n",
    "\n",
    "# ============== CREAR MODELO CON TRANSFER LEARNING ==============\n",
    "print(\"\\nCreando modelo con MobileNetV2...\")\n",
    "\n",
    "# Cargar modelo base preentrenado\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Congelar las capas del modelo base inicialmente\n",
    "base_model.trainable = False\n",
    "\n",
    "# Crear el modelo completo\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "], name='AI_Image_Detector')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ============== COMPILAR MODELO ==============\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# ============== CALLBACKS ==============\n",
    "callbacks = [\n",
    "    # Detener si no mejora en 7 √©pocas\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Guardar el mejor modelo\n",
    "    ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Reducir learning rate si se estanca\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ============== ENTRENAMIENTO ==============\n",
    "print(\"\\n========== FASE 1: Entrenamiento con capas congeladas ==========\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============== FINE-TUNING ==============\n",
    "print(\"\\n========== FASE 2: Fine-tuning (descongelando capas) ==========\")\n",
    "\n",
    "# Descongelar las √∫ltimas capas del modelo base\n",
    "base_model.trainable = True\n",
    "\n",
    "# Congelar solo las primeras capas\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompilar con learning rate m√°s bajo\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# Continuar entrenamiento\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    initial_epoch=len(history.history['loss']),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Combinar historiales\n",
    "for key in history.history.keys():\n",
    "    history.history[key].extend(history_fine.history[key])\n",
    "\n",
    "# ============== EVALUACI√ìN ==============\n",
    "print(\"\\n========== EVALUACI√ìN EN TEST SET ==========\")\n",
    "test_loss, test_acc, test_auc = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Guardar modelo final\n",
    "model.save('final_model.keras')\n",
    "print(\"\\nModelo guardado como 'final_model.keras'\")\n",
    "\n",
    "# ============== VISUALIZACI√ìN DE RESULTADOS ==============\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], 'b', label='Train')\n",
    "axes[0].plot(history.history['val_accuracy'], 'r', label='Validation')\n",
    "axes[0].set_xlabel('√âpocas')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy durante entrenamiento')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], 'b', label='Train')\n",
    "axes[1].plot(history.history['val_loss'], 'r', label='Validation')\n",
    "axes[1].set_xlabel('√âpocas')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Loss durante entrenamiento')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ============== FUNCI√ìN DE PREDICCI√ìN ==============\n",
    "def predict_image(img_path, model, class_names):\n",
    "    \"\"\"\n",
    "    Predice si una imagen es real o generada por IA\n",
    "    \n",
    "    Args:\n",
    "        img_path: Ruta de la imagen\n",
    "        model: Modelo entrenado\n",
    "        class_names: Lista con nombres de las clases\n",
    "        \n",
    "    Returns:\n",
    "        predicted_class: Clase predicha\n",
    "        confidence: Confianza de la predicci√≥n (%)\n",
    "    \"\"\"\n",
    "    # Cargar y preprocesar imagen\n",
    "    img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Predecir\n",
    "    prediction = model.predict(img_array, verbose=0)\n",
    "    predicted_idx = np.argmax(prediction)\n",
    "    predicted_class = class_names[predicted_idx]\n",
    "    confidence = prediction[0][predicted_idx] * 100\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Predicci√≥n: {predicted_class}\\nConfianza: {confidence:.2f}%\", \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # A√±adir barra de probabilidades\n",
    "    plt.text(10, img.size[1] - 10, \n",
    "             f\"{class_names[0]}: {prediction[0][0]*100:.1f}%\\n{class_names[1]}: {prediction[0][1]*100:.1f}%\",\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "             fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Imagen: {img_path}\")\n",
    "    print(f\"Predicci√≥n: {predicted_class}\")\n",
    "    print(f\"Confianza: {confidence:.2f}%\")\n",
    "    print(f\"Probabilidades detalladas:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"  - {class_name}: {prediction[0][i]*100:.2f}%\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# ============== PREDICCIONES DE EJEMPLO ==============\n",
    "print(\"\\n========== PREDICCIONES EN IM√ÅGENES DE PRUEBA ==========\")\n",
    "\n",
    "# Lista de im√°genes a probar\n",
    "test_images = [\n",
    "    r\"C:\\Users\\Usuario\\Downloads\\IA1.jpg\",\n",
    "    r\"C:\\Users\\Usuario\\Downloads\\IA2.jpg\",\n",
    "    r\"C:\\Users\\Usuario\\Downloads\\IA3.jpg\",\n",
    "    r\"C:\\Users\\Usuario\\Downloads\\IA4.jpg\"\n",
    "]\n",
    "\n",
    "# Predecir cada imagen\n",
    "results = []\n",
    "for img_path in test_images:\n",
    "    try:\n",
    "        predicted_class, confidence = predict_image(img_path, model, class_names)\n",
    "        results.append({\n",
    "            'path': img_path,\n",
    "            'prediction': predicted_class,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar {img_path}: {e}\")\n",
    "\n",
    "# Resumen de resultados\n",
    "print(\"\\n========== RESUMEN DE PREDICCIONES ==========\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['path'].split('\\\\')[-1]}: {result['prediction']} ({result['confidence']:.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úì Proceso completado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e64de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyectoIA_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
